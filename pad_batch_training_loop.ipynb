{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Custom imports\n",
    "import bert_nbme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define globals\n",
    "CONFIG = 'bert-base-uncased'\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'DEVICE: {DEVICE}')\n",
    "\n",
    "# Import data\n",
    "notes_df = pd.read_csv('data/patient_notes.csv')\n",
    "train_df = pd.read_csv('data/train.csv')\n",
    "features_df = pd.read_csv('data/features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "none_row = pd.DataFrame({'feature_num': [-1], 'case_num': [-1], 'feature_text': ['NONE']}, index=[len(features_df)])\n",
    "features_df = pd.concat((features_df, none_row))  # Add NONE value as a feature\n",
    "features_df['feature_index'] = range(len(features_df))\n",
    "\n",
    "# APPEND AND CLEAN DATA\n",
    "data = train_df[train_df['annotation'] != '[]']  # Drop blank annotations ('[]')\n",
    "data['annotation'] = [i.translate(i.maketrans('', '', '[]\\'')).split(' ') for i in data['annotation']]\n",
    "data = data.merge(features_df[['feature_num', 'feature_text', 'feature_index']], on='feature_num')  # Add features\n",
    "data = data.merge(notes_df[['pn_num', 'pn_history']], on='pn_num')  # Add notes\n",
    "data = data.dropna().reset_index(drop=True)  # Drop and reindex any leftover trouble-makers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['pn_history'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize word lists\n",
    "tokenizer = BertTokenizer.from_pretrained(CONFIG)\n",
    "encoded_word_lists = [tokenizer.encode(x) for x in data['pn_history']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def substring_loc(input_str, substring):\n",
    "    start_ind = input_str.find(substring)\n",
    "    end_ind = start_ind + len(substring)\n",
    "    return start_ind, end_ind\n",
    "    \n",
    "\n",
    "test = 'ape is my middle name'\n",
    "sub = 'is my middle'\n",
    "i = test.find(sub)\n",
    "test[i:len(sub) + i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.location\n",
    "data.pn_history.iloc[0][696:724]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max([len(x) for x in data.pn_history])\n",
    "data.pn_history.where(data.pn_history.str.len() > 949).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "81daf03f63ef07d445e854f4df165e1f63c8fd7699578242bc4bfe0d2d2e24db"
  },
  "kernelspec": {
   "display_name": "Python 3.9.11 ('nbme')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
