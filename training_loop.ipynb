{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overview:\n",
    "\n",
    "Each of the 143 features are assigned a unique class and a 144th feature is added to represent no class. Each word will be classified as whether or not it has relation to the target class based on its existance in the annotations. Patient notes are encoded using the BERT base-uncased tokenizer (integer tokens representing dictionary index). tokens are fed, one at a time, into a BERT model producing a 768 hidden weight vector for each word in a patient note. The entire note represents one batch. The weight vector is fed into a fully connected linear layer which ouputs a vector with length equivalent to the number of classes for each word. This is then run through a softmax output layer and cross entropy loss is computed between the softmax wight vector and the integer class number (index in softmax).\n",
    "\n",
    "Cleaning:\n",
    "1. Rows of the training dataframe with blank notes are removed.\n",
    "2. '[]' encases each note, these are removed.\n",
    "3. Notes split into lists on ' '.\n",
    "\n",
    "\n",
    "Dataflow:\n",
    "1. Raw patient notes --> cleaned patient notes\n",
    "2. Cleaned patient notes --> BERT tokenized strings\n",
    "3. BERT tokenized strings --> list of BERT tokenized words (split on ' ')\n",
    "4. List of BERT tokenized words --BERT-model--> 768 dimension hidden vector\n",
    "5. 768 dimension hidden vector --linear-layer--> {CLASS_NUM} dimension vector\n",
    "6. {CLASS_NUM} dimension vector --softmax--> weighted vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Custom imports\n",
    "import bert_nbme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define globals\n",
    "CONFIG = 'bert-base-uncased'\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'DEVICE: {DEVICE}')\n",
    "\n",
    "# Import data\n",
    "notes_df = pd.read_csv('data/patient_notes.csv')\n",
    "train_df = pd.read_csv('data/train.csv')\n",
    "features_df = pd.read_csv('data/features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "none_row = pd.DataFrame({'feature_num': [-1], 'case_num': [-1], 'feature_text': ['NONE']}, index=[len(features_df)])\n",
    "features_df = pd.concat((features_df, none_row))  # Add NONE value as a feature\n",
    "features_df['feature_index'] = range(len(features_df))\n",
    "\n",
    "# APPEND AND CLEAN DATA\n",
    "data = train_df[train_df['annotation'] != '[]']  # Drop blank annotations ('[]')\n",
    "data['annotation'] = [i.translate(i.maketrans('', '', '[]\\'')).split(' ') for i in data['annotation']]\n",
    "data = data.merge(features_df[['feature_num', 'feature_text', 'feature_index']], on='feature_num')  # Add features\n",
    "data = data.merge(notes_df[['pn_num', 'pn_history']], on='pn_num')  # Add notes\n",
    "# seps = [' ', ',', ';', ':', '.', '!', '?', '-', '_', '\\n']  # WORRY ABOUT THIS LATER\n",
    "word_lists = data['pn_history'].apply(lambda x: np.array(x.split(' '))).to_numpy()  # Convert notes to lists of words\n",
    "data = data.dropna().reset_index(drop=True)  # Drop and reindex any leftover trouble-makers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "none_ind = len(features_df) - 1  # Vector value for NONE\n",
    "y = []\n",
    "for i, note in enumerate(word_lists):\n",
    "    word_labels = [none_ind]  # Pad first with NONE bc start token [CLS] added\n",
    "    for word in note:\n",
    "        if word in data['annotation'].iloc[i]:\n",
    "            word_labels.append(data['feature_index'].iloc[i])\n",
    "        else:\n",
    "            word_labels.append(none_ind)\n",
    "    word_labels.append(none_ind)  # Pad last with NONE bc end token [SEP] added\n",
    "    y.append(torch.cuda.LongTensor(word_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize word lists\n",
    "tokenizer = BertTokenizer.from_pretrained(CONFIG)\n",
    "encoded_word_lists = [tokenizer.encode(x.tolist()) for x in word_lists]\n",
    "\n",
    "# Cast features to tensors\n",
    "X = [torch.cuda.IntTensor(np.array(x).reshape(1, -1)) for x in encoded_word_lists]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model params\n",
    "LEARNING_RATE = 10\n",
    "EPOCHS = 10\n",
    "\n",
    "# Loss function, model, and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model = bert_nbme.BertNN(num_classes=len(features_df), bert_config=CONFIG).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(lr=LEARNING_RATE, params=model.parameters())\n",
    "\n",
    "# Loss history over epochs for plotting\n",
    "train_loss_history = []\n",
    "val_loss_history = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f'EPOCH: {epoch}')\n",
    "    \n",
    "    # Initialize single-epoch loss\n",
    "    epoch_train_loss = []\n",
    "    epoch_val_loss = []\n",
    "    \n",
    "    for note, target in zip(X, y):\n",
    "        # Zero out gradient every batch\n",
    "        model.zero_grad()\n",
    "        \n",
    "        # Make predictions\n",
    "        pred = model(note)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(pred, target)\n",
    "        \n",
    "        # Take train step\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Compile loss\n",
    "        epoch_train_loss.append(loss.item())\n",
    "    \n",
    "    # Append average loss over epoch to history\n",
    "    train_loss_history.append(sum(epoch_train_loss) / len(epoch_train_loss))\n",
    "    print(f'LOSS: {train_loss_history[-1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss over epochs\n",
    "plt.figure()\n",
    "plt.title('Cross Entropy Loss')\n",
    "plt.plot(range(EPOCHS), train_loss_history, label='Train loss', color='r', lw=3)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch = 1 note\n",
    "Sample = 1 word\n",
    "\n",
    "each word needs its own vector (MOST WILL BE NONE)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "81daf03f63ef07d445e854f4df165e1f63c8fd7699578242bc4bfe0d2d2e24db"
  },
  "kernelspec": {
   "display_name": "Python 3.9.11 ('nbme')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
