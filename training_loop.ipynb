{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Custom imports\n",
    "from stse.bytes import bit_vect\n",
    "import bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Define globals\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "CONFIG = 'bert-large-uncased'\n",
    "\n",
    "# Define BERT model\n",
    "bert_model = BertModel.from_pretrained(CONFIG).to(DEVICE)\n",
    "\n",
    "# Import data\n",
    "notes_df = pd.read_csv('data/patient_notes.csv')\n",
    "train_df = pd.read_csv('data/train.csv')\n",
    "features_df = pd.read_csv('data/features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jwger\\AppData\\Local\\Temp\\ipykernel_42808\\1220022056.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['annotation'] = [i.translate(i.maketrans('', '', '[]\\'')).split(' ') for i in data['annotation']]\n"
     ]
    }
   ],
   "source": [
    "# One-hot encode features\n",
    "features_df['feature_vect'] = [bit_vect(len(features_df) + 1, i) for i in range(len(features_df['feature_text']))]\n",
    "none_row = pd.DataFrame({\n",
    "        'feature_num': [-1],\n",
    "        'case_num': [-1],\n",
    "        'feature_text': ['NONE'],\n",
    "        'feature_vect': [bit_vect(len(features_df) + 1, len(features_df))]\n",
    "    }, index=[len(features_df)])\n",
    "features_df = pd.concat((features_df, none_row))  # Add NONE value as a feature\n",
    "\n",
    "# APPEND AND CLEAN DATA\n",
    "data = train_df[train_df['annotation'] != '[]']  # Drop blank annotations ('[]')\n",
    "data['annotation'] = [i.translate(i.maketrans('', '', '[]\\'')).split(' ') for i in data['annotation']]\n",
    "data = data.merge(features_df[['feature_num', 'feature_text', 'feature_vect']], on='feature_num')  # Add features\n",
    "data = data.merge(notes_df[['pn_num', 'pn_history']], on='pn_num')  # Add notes\n",
    "# seps = [' ', ',', ';', ':', '.', '!', '?', '-', '_', '\\n']  # WORRY ABOUT THIS LATER\n",
    "word_lists = data['pn_history'].apply(lambda x: np.array(x.split(' '))).to_numpy()  # Convert notes to lists of words\n",
    "data = data.dropna().reset_index(drop=True)  # Drop and reindex any leftover trouble-makers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_labels = []\n",
    "for i, note in enumerate(word_lists):\n",
    "    word_labels = []\n",
    "    for word in note:\n",
    "        if word in data['annotation'].iloc[i]:\n",
    "            word_labels.append(data['feature_vect'].iloc[i])\n",
    "        else:\n",
    "            word_labels.append(features_df['feature_vect'].iloc[-1])  # Value for NONE\n",
    "    total_labels.append(np.array(word_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize word lists\n",
    "tokenizer = BertTokenizer.from_pretrained(CONFIG)\n",
    "encoded_word_lists = [tokenizer.encode(x.tolist()) for x in word_lists]\n",
    "\n",
    "# Cast features and labels to tensors\n",
    "X = [torch.cuda.IntTensor(np.array(x).reshape(1, -1)) for x in encoded_word_lists][0:5]\n",
    "# y = [torch.cuda.ByteTensor(x) for x in data['feature_vect'].to_numpy()]\n",
    "y = [torch.cuda.ByteTensor(x) for x in total_labels][0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([144]), torch.Size([1, 144]))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0][0].shape, X[0].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = bert.BertDataset(X, y)\n",
    "x , y = dataset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 100, 100, 100, 7534, 2007, 100, 100, 4311, 100, 2706, 1997, 23852, 4178, 1997, 100, 100, 2041, 1997, 2026, 100, 1016, 2420, 3283, 2076, 1037, 4715, 2208, 2018, 2019, 100, 2021, 2023, 2051, 2018, 3108, 3778, 1998, 2371, 2004, 2065, 2002, 2020, 2183, 2000, 3413, 2041, 100, 2025, 4558, 100, 100, 3602, 5776, 100, 100, 100, 3952, 2000, 2817, 100, 2335, 2566, 100, 100, 3522, 4715, 100, 2165, 100, 2305, 2077, 1998, 2851, 1997, 100, 100, 100, 1997, 100, 100, 100, 100, 100, 100, 3431, 1999, 100, 3431, 1999, 100, 21419, 100, 3431, 1999, 100, 2030, 100, 100, 100, 100, 3594, 2814, 100, 3566, 2007, 100, 100, 3611, 2007, 3522, 2540, 100, 100, 2039, 2000, 100, 100, 1999, 100, 100, 100, 8974, 1017, 6385, 1013, 2733, 100, 100, 23439, 100, 100, 2667, 100, 100, 3161, 2007, 6513, 1060, 1015, 100, 3594, 29094, 102]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[CLS] [UNK] [UNK] [UNK] presents with [UNK] [UNK] reports [UNK] months of intermittent episodes of [UNK] [UNK] out of my [UNK] 2 days ago during a soccer game had an [UNK] but this time had chest pressure and felt as if he were going to pass out [UNK] not lose [UNK] [UNK] note patient [UNK] [UNK] [UNK] primarily to study [UNK] times per [UNK] [UNK] recent soccer [UNK] took [UNK] night before and morning of [UNK] [UNK] [UNK] of [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] changes in [UNK] changes in [UNK] abdominal [UNK] changes in [UNK] or [UNK] [UNK] [UNK] [UNK] uses friends [UNK] mom with [UNK] [UNK] dad with recent heart [UNK] [UNK] up to [UNK] [UNK] in [UNK] [UNK] [UNK] drinks 3 nights / week [UNK] [UNK] denies [UNK] [UNK] trying [UNK] [UNK] active with girlfriend x 1 [UNK] uses condoms [SEP]'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x\n",
    "len(word_lists[0])\n",
    "print(encoded_word_lists[0])\n",
    "tokenizer.decode(encoded_word_lists[0])\n",
    "# torch.cuda.IntTensor(np.array(word_lists[0]).reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 0\n",
      "torch.Size([1, 144, 144])\n",
      "torch.Size([142, 144])\n",
      "tensor([[[0.0027, 0.0078, 0.0053,  ..., 0.0064, 0.0063, 0.0073],\n",
      "         [0.0074, 0.0074, 0.0101,  ..., 0.0061, 0.0073, 0.0075],\n",
      "         [0.0078, 0.0076, 0.0075,  ..., 0.0069, 0.0083, 0.0073],\n",
      "         ...,\n",
      "         [0.0050, 0.0085, 0.0049,  ..., 0.0061, 0.0063, 0.0062],\n",
      "         [0.0059, 0.0077, 0.0057,  ..., 0.0080, 0.0065, 0.0061],\n",
      "         [0.0076, 0.0062, 0.0044,  ..., 0.0077, 0.0048, 0.0130]]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 1],\n",
      "        [0, 0, 0,  ..., 0, 0, 1],\n",
      "        [0, 0, 0,  ..., 0, 0, 1],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 1],\n",
      "        [0, 0, 0,  ..., 0, 0, 1],\n",
      "        [0, 0, 0,  ..., 0, 0, 1]], device='cuda:0', dtype=torch.uint8)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (1) to match target batch_size (142).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\jwger\\PycharmProjects\\kaggle_nbme_patient_notes\\training_loop.ipynb Cell 7'\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jwger/PycharmProjects/kaggle_nbme_patient_notes/training_loop.ipynb#ch0000004?line=30'>31</a>\u001b[0m \u001b[39mprint\u001b[39m(target)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jwger/PycharmProjects/kaggle_nbme_patient_notes/training_loop.ipynb#ch0000004?line=31'>32</a>\u001b[0m \u001b[39m# Calculate loss\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/jwger/PycharmProjects/kaggle_nbme_patient_notes/training_loop.ipynb#ch0000004?line=32'>33</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(pred, target)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jwger/PycharmProjects/kaggle_nbme_patient_notes/training_loop.ipynb#ch0000004?line=34'>35</a>\u001b[0m \u001b[39m# Take train step\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jwger/PycharmProjects/kaggle_nbme_patient_notes/training_loop.ipynb#ch0000004?line=35'>36</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nbme\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/jwger/anaconda3/envs/nbme/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/jwger/anaconda3/envs/nbme/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/jwger/anaconda3/envs/nbme/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/jwger/anaconda3/envs/nbme/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/jwger/anaconda3/envs/nbme/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/jwger/anaconda3/envs/nbme/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/jwger/anaconda3/envs/nbme/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nbme\\lib\\site-packages\\torch\\nn\\modules\\loss.py:1163\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/jwger/anaconda3/envs/nbme/lib/site-packages/torch/nn/modules/loss.py?line=1161'>1162</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m-> <a href='file:///c%3A/Users/jwger/anaconda3/envs/nbme/lib/site-packages/torch/nn/modules/loss.py?line=1162'>1163</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mcross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[0;32m   <a href='file:///c%3A/Users/jwger/anaconda3/envs/nbme/lib/site-packages/torch/nn/modules/loss.py?line=1163'>1164</a>\u001b[0m                            ignore_index\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mignore_index, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction,\n\u001b[0;32m   <a href='file:///c%3A/Users/jwger/anaconda3/envs/nbme/lib/site-packages/torch/nn/modules/loss.py?line=1164'>1165</a>\u001b[0m                            label_smoothing\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabel_smoothing)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nbme\\lib\\site-packages\\torch\\nn\\functional.py:2996\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/jwger/anaconda3/envs/nbme/lib/site-packages/torch/nn/functional.py?line=2993'>2994</a>\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   <a href='file:///c%3A/Users/jwger/anaconda3/envs/nbme/lib/site-packages/torch/nn/functional.py?line=2994'>2995</a>\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> <a href='file:///c%3A/Users/jwger/anaconda3/envs/nbme/lib/site-packages/torch/nn/functional.py?line=2995'>2996</a>\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mcross_entropy_loss(\u001b[39minput\u001b[39;49m, target, weight, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[1;31mValueError\u001b[0m: Expected input batch_size (1) to match target batch_size (142)."
     ]
    }
   ],
   "source": [
    "# Model params\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCHS = 10\n",
    "\n",
    "# Loss function, model, and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model = bert.BertBased(num_classes=144, bert_config=CONFIG).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(lr=LEARNING_RATE, params=model.parameters())\n",
    "\n",
    "# Loss history over epochs for plotting\n",
    "train_loss_history = []\n",
    "val_loss_history = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f'EPOCH: {epoch}')\n",
    "    \n",
    "    # Initialize single-epoch loss\n",
    "    epoch_train_loss = []\n",
    "    epoch_val_loss = []\n",
    "    \n",
    "    for note, target in zip(X, y):\n",
    "        # Zero out gradient every batch\n",
    "        model.zero_grad()\n",
    "        \n",
    "        # Make predictions\n",
    "        pred = model(note)\n",
    "        print(pred.shape)\n",
    "        print(target.shape)\n",
    "        \n",
    "        print(pred)\n",
    "        print(target)\n",
    "        # Calculate loss\n",
    "        loss = criterion(pred, target)\n",
    "        \n",
    "        # Take train step\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Compile loss\n",
    "        epoch_train_loss.append(loss.item())\n",
    "    \n",
    "    # Append average loss over epoch to history\n",
    "    train_loss_history.append(sum(epoch_train_loss) / len(epoch_train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss over epochs\n",
    "plt.figure()\n",
    "plt.title('Cross Entropy Loss')\n",
    "plt.plot(range(EPOCHS), train_loss_history, label='Train loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch = 1 note\n",
    "Sample = 1 word\n",
    "\n",
    "each word needs its own vector (MOST WILL BE NONE)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "81daf03f63ef07d445e854f4df165e1f63c8fd7699578242bc4bfe0d2d2e24db"
  },
  "kernelspec": {
   "display_name": "Python 3.9.11 ('nbme')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
